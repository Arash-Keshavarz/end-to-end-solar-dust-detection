{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f11ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1abb241",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf48aabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/arash/ML_End_to_End_Pj/end-to-end-solar-dust-detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8205535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_learning_rate: float\n",
    "    params_classes: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f134e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solar_dust_detection.constants import *\n",
    "from solar_dust_detection.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "924fed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: Path = CONFIG_FILE_PATH,\n",
    "        params_filepath: Path = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([Path(self.config.artifacts_root)])\n",
    "        \n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training_config = self.config.training\n",
    "\n",
    "        root_dir = Path(training_config.root_dir)\n",
    "        trained_model_path = Path(training_config.trained_model_path)\n",
    "        updated_base_model_path = Path(self.config.base_model.updated_base_model_path)\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzipped_data_dir, \"Detect_solar_dust\")\n",
    "        create_directories([root_dir])\n",
    "        \n",
    "        params_epochs = self.params.EPOCHS\n",
    "        params_batch_size = self.params.BATCH_SIZE\n",
    "        params_is_augmentation = self.params.AUGMENTATION\n",
    "        params_image_size = self.params.IMAGE_SIZE\n",
    "        params_learning_rate = self.params.LEARNING_RATE\n",
    "        params_classes = self.params.CLASSES\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=root_dir,\n",
    "            trained_model_path=trained_model_path,\n",
    "            updated_base_model_path=updated_base_model_path,\n",
    "            training_data=training_data,\n",
    "            params_epochs=params_epochs,\n",
    "            params_batch_size=params_batch_size,\n",
    "            params_is_augmentation=params_is_augmentation,\n",
    "            params_image_size=params_image_size,\n",
    "            params_learning_rate=params_learning_rate,\n",
    "            params_classes=params_classes,\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a4a9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components update\n",
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, datasets, models\n",
    "from solar_dust_detection import logger\n",
    "import time\n",
    "\n",
    "class MapDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.dataset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        \n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, self.config.params_classes)\n",
    "        \n",
    "        checkpoint = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint)\n",
    "        \n",
    "        # 4. Move to GPU/CPU\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def train_valid_generator(self):\n",
    "        \n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        val_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.config.params_image_size[:-1]), # Resize to (224, 224)\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "        if self.config.params_is_augmentation:\n",
    "            train_transforms = transforms.Compose([\n",
    "                transforms.Resize(self.config.params_image_size[:-1]),\n",
    "                transforms.RandomRotation(40),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), shear=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "        else:\n",
    "            train_transforms = val_transforms       \n",
    "\n",
    "        # ImageFolder expects structure: data/class_a/img1.jpg, data/class_b/img2.jpg\n",
    "        full_dataset = datasets.ImageFolder(root=self.config.training_data)\n",
    "        \n",
    "        val_size = int(len(full_dataset) * 0.20)\n",
    "        train_size = len(full_dataset) - val_size\n",
    "        \n",
    "        train_subset, val_subset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "        train_dataset = MapDataset(train_subset, train_transforms)\n",
    "        val_dataset = MapDataset(val_subset, val_transforms)\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.params_batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0 \n",
    "        )\n",
    "        \n",
    "        self.valid_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config.params_batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: nn.Module):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "    def train(self):\n",
    "        # 1. Define Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "\n",
    "        print(f\"Training on {self.device} with {len(self.train_loader.dataset)} samples.\")\n",
    "\n",
    "        # 2. The Training Loop \n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            self.model.train() \n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            # --- Batch Loop ---\n",
    "            for images, labels in self.train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Metrics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # --- Epoch Metrics ---\n",
    "            epoch_acc = 100 * correct / total\n",
    "            logger.info(f\"Epoch [{epoch+1}/{self.config.params_epochs}] \"\n",
    "                  f\"Loss: {running_loss/len(self.train_loader):.4f} \"\n",
    "                  f\"Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "        logger.info(f\"Model saved to {self.config.trained_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1362ed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-24 17:07:15,760: INFO: common]: YAML file: config/config.yaml loaded successfully\n",
      "[2026-01-24 17:07:15,766: INFO: common]: YAML file: params.yaml loaded successfully\n",
      "[2026-01-24 17:07:15,767: INFO: common]: Created directory at: artifacts\n",
      "[2026-01-24 17:07:15,768: INFO: common]: Created directory at: artifacts/training\n",
      "[2026-01-24 17:07:15,768: INFO: 31722983]: Using device: mps\n",
      "Training on mps with 2050 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arash/ML_End_to_End_Pj/end-to-end-solar-dust-detection/.venv/lib/python3.12/site-packages/PIL/Image.py:1034: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-24 17:08:13,527: INFO: 31722983]: Epoch [1/40] Loss: 0.5429 Acc: 72.59%\n",
      "[2026-01-24 17:08:59,534: INFO: 31722983]: Epoch [2/40] Loss: 0.4928 Acc: 78.15%\n",
      "[2026-01-24 17:09:45,355: INFO: 31722983]: Epoch [3/40] Loss: 0.4056 Acc: 82.59%\n",
      "[2026-01-24 17:10:32,885: INFO: 31722983]: Epoch [4/40] Loss: 0.3734 Acc: 84.20%\n",
      "[2026-01-24 17:11:19,864: INFO: 31722983]: Epoch [5/40] Loss: 0.3519 Acc: 85.71%\n",
      "[2026-01-24 17:12:06,006: INFO: 31722983]: Epoch [6/40] Loss: 0.3365 Acc: 85.37%\n",
      "[2026-01-24 17:12:52,556: INFO: 31722983]: Epoch [7/40] Loss: 0.2734 Acc: 88.88%\n",
      "[2026-01-24 17:13:42,026: INFO: 31722983]: Epoch [8/40] Loss: 0.2982 Acc: 88.24%\n",
      "[2026-01-24 17:14:28,194: INFO: 31722983]: Epoch [9/40] Loss: 0.2786 Acc: 88.34%\n",
      "[2026-01-24 17:15:14,797: INFO: 31722983]: Epoch [10/40] Loss: 0.2653 Acc: 89.02%\n",
      "[2026-01-24 17:16:00,875: INFO: 31722983]: Epoch [11/40] Loss: 0.2158 Acc: 92.00%\n",
      "[2026-01-24 17:16:47,174: INFO: 31722983]: Epoch [12/40] Loss: 0.2234 Acc: 91.22%\n",
      "[2026-01-24 17:17:33,470: INFO: 31722983]: Epoch [13/40] Loss: 0.1888 Acc: 92.20%\n",
      "[2026-01-24 17:18:19,956: INFO: 31722983]: Epoch [14/40] Loss: 0.1752 Acc: 93.27%\n",
      "[2026-01-24 17:19:07,122: INFO: 31722983]: Epoch [15/40] Loss: 0.1896 Acc: 92.44%\n",
      "[2026-01-24 17:19:53,866: INFO: 31722983]: Epoch [16/40] Loss: 0.1891 Acc: 92.20%\n",
      "[2026-01-24 17:20:41,342: INFO: 31722983]: Epoch [17/40] Loss: 0.1693 Acc: 94.20%\n",
      "[2026-01-24 17:21:28,833: INFO: 31722983]: Epoch [18/40] Loss: 0.2141 Acc: 92.63%\n",
      "[2026-01-24 17:22:14,672: INFO: 31722983]: Epoch [19/40] Loss: 0.1582 Acc: 94.24%\n",
      "[2026-01-24 17:23:00,147: INFO: 31722983]: Epoch [20/40] Loss: 0.1337 Acc: 94.68%\n",
      "[2026-01-24 17:23:45,848: INFO: 31722983]: Epoch [21/40] Loss: 0.1460 Acc: 94.49%\n",
      "[2026-01-24 17:24:32,355: INFO: 31722983]: Epoch [22/40] Loss: 0.1701 Acc: 92.98%\n",
      "[2026-01-24 17:25:20,121: INFO: 31722983]: Epoch [23/40] Loss: 0.1310 Acc: 94.54%\n",
      "[2026-01-24 17:26:09,196: INFO: 31722983]: Epoch [24/40] Loss: 0.1288 Acc: 94.83%\n",
      "[2026-01-24 17:26:56,034: INFO: 31722983]: Epoch [25/40] Loss: 0.1412 Acc: 95.02%\n",
      "[2026-01-24 17:27:43,300: INFO: 31722983]: Epoch [26/40] Loss: 0.1680 Acc: 94.05%\n",
      "[2026-01-24 17:28:43,367: INFO: 31722983]: Epoch [27/40] Loss: 0.1143 Acc: 95.61%\n",
      "[2026-01-24 17:29:43,005: INFO: 31722983]: Epoch [28/40] Loss: 0.1058 Acc: 95.85%\n",
      "[2026-01-24 17:30:34,676: INFO: 31722983]: Epoch [29/40] Loss: 0.0837 Acc: 96.98%\n",
      "[2026-01-24 17:31:29,578: INFO: 31722983]: Epoch [30/40] Loss: 0.0959 Acc: 96.83%\n",
      "[2026-01-24 17:32:27,116: INFO: 31722983]: Epoch [31/40] Loss: 0.0978 Acc: 96.34%\n",
      "[2026-01-24 17:33:16,364: INFO: 31722983]: Epoch [32/40] Loss: 0.0875 Acc: 96.73%\n",
      "[2026-01-24 17:34:06,142: INFO: 31722983]: Epoch [33/40] Loss: 0.0979 Acc: 96.88%\n",
      "[2026-01-24 17:35:02,577: INFO: 31722983]: Epoch [34/40] Loss: 0.1208 Acc: 95.46%\n",
      "[2026-01-24 17:35:56,399: INFO: 31722983]: Epoch [35/40] Loss: 0.0809 Acc: 96.88%\n",
      "[2026-01-24 17:36:47,271: INFO: 31722983]: Epoch [36/40] Loss: 0.0966 Acc: 97.41%\n",
      "[2026-01-24 17:37:38,918: INFO: 31722983]: Epoch [37/40] Loss: 0.0887 Acc: 96.15%\n",
      "[2026-01-24 17:41:20,353: INFO: 31722983]: Epoch [38/40] Loss: 0.0833 Acc: 96.83%\n",
      "[2026-01-24 17:43:09,870: INFO: 31722983]: Epoch [39/40] Loss: 0.0643 Acc: 97.46%\n",
      "[2026-01-24 17:44:40,366: INFO: 31722983]: Epoch [40/40] Loss: 0.0589 Acc: 97.85%\n",
      "[2026-01-24 17:44:40,719: INFO: 31722983]: Model saved to artifacts/training/model.pt\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d176b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9261a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
